{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaff197-e022-45b1-8b0b-a68f620da6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4083d1db-875c-48d1-9537-336062c70f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "annotations_directory_path = 'eccv_18_annotation_files/'\n",
    "train_annotations = load_annotations(f'{annotations_directory_path}train_annotations.json')\n",
    "test_annotations = load_annotations(f'{annotations_directory_path}cis_test_annotations.json')\n",
    "val_annotations = load_annotations(f'{annotations_directory_path}cis_val_annotations.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff376035-8a3d-42bd-a212-67745fd3cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations, images_directory_path, transform=None):\n",
    "        self.images_directory_path = images_directory_path\n",
    "        self.transform = transform\n",
    "        self.annotations = annotations\n",
    "\n",
    "        # Mapping from image ID to image information\n",
    "        self.image_id_to_info = {image['id']: image for image in annotations['images']}\n",
    "\n",
    "        # Mapping from category ID to category name\n",
    "        self.category_id_to_name = {category['id']: category['name'] for category in annotations['categories']}\n",
    "\n",
    "        # Assuming each image has only one annotation for simplicity. If there are multiple, this needs adjustment.\n",
    "        self.image_id_to_annotation = {annotation['image_id']: annotation for annotation in annotations['annotations']}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_info = self.annotations['images'][idx]\n",
    "        img_id = image_info['id']\n",
    "        img_path = os.path.join(self.images_directory_path, image_info['file_name'])\n",
    "    \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found: {img_path}\")\n",
    "            return None\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    \n",
    "        # Handle multiple annotations\n",
    "        annotations = [ann for ann in self.annotations['annotations'] if ann['image_id'] == img_id]\n",
    "    \n",
    "        # Prepare targets for DETR (boxes and labels)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            bbox = ann.get('bbox', [0, 0, 0, 0])\n",
    "            # Convert COCO bbox format from [x, y, width, height] to [x1, y1, x2, y2]\n",
    "            bbox = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "            boxes.append(bbox)\n",
    "            labels.append(self.category_id_to_name.get(ann['category_id'], -1))\n",
    "    \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "    \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "    \n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4cfd2d8-923b-4632-a9b4-c4166b48f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your images are stored in a single directory\n",
    "images_directory_path = 'eccv_18_all_images_sm/'\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = CustomDataset(test_annotations, images_directory_path, transform = transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c8e320-338d-40f5-b43a-80df15953733",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Example usage: print the first 5 images from your DataLoader\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mprint_first_n_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mprint_first_n_images\u001b[0;34m(data_loader, num_images, denormalize)\u001b[0m\n\u001b[1;32m     12\u001b[0m figure, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ncols\u001b[38;5;241m=\u001b[39mnum_images, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# Adjust the size accordingly\u001b[39;00m\n\u001b[1;32m     13\u001b[0m image_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images)):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m image_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_images:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[2], line 54\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory_id_to_name\u001b[38;5;241m.\u001b[39mget(ann[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     53\u001b[0m boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(boxes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 54\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m target \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     57\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def print_first_n_images(data_loader, num_images=10, denormalize=True):\n",
    "    \"\"\"\n",
    "    Prints the first `num_images` images from the DataLoader in a grid.\n",
    "    Args:\n",
    "    data_loader (DataLoader): The DataLoader from which to fetch images.\n",
    "    num_images (int): Number of images to print.\n",
    "    denormalize (bool): Whether to denormalize images if they were normalized.\n",
    "    \"\"\"\n",
    "    figure, ax = plt.subplots(nrows=1, ncols=num_images, figsize=(15, 3))  # Adjust the size accordingly\n",
    "    image_count = 0\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        for i in range(len(images)):\n",
    "            if image_count >= num_images:\n",
    "                plt.show()\n",
    "                return\n",
    "\n",
    "            img = images[i].numpy().transpose((1, 2, 0))\n",
    "\n",
    "            if denormalize:\n",
    "                # Assuming normalization mean and std for ImageNet if used commonly\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "\n",
    "            ax[image_count].imshow(img)\n",
    "            ax[image_count].set_title(f\"Label: {targets['labels'][i].item()}\")\n",
    "            ax[image_count].axis('off')\n",
    "\n",
    "            image_count += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: print the first 5 images from your DataLoader\n",
    "print_first_n_images(train_loader, num_images=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be7be9c8-af0c-4262-9f6a-94dd4d1ffd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93f1e06-a8a8-454b-978f-982827c8d21f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'detr_resnet50' from 'torchvision.models.detection' (/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detr_resnet50\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m detr_resnet50(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m21\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'detr_resnet50' from 'torchvision.models.detection' (/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import detr_resnet50\n",
    "\n",
    "# Initialize the model\n",
    "model = detr_resnet50(pretrained=True, num_classes=21)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Assume custom Dataset class for Camera Traps data\n",
    "from your_dataset_module import CameraTrapsDataset\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = CameraTrapsDataset('path_to_train_data', transforms=your_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Training loop (simplified)\n",
    "for epoch in range(num_epochs):\n",
    "    for images, targets in train_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4e7f0f7-e4c6-4f26-a487-5cb053e53e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b23641e4-92b6-45c7-9084-4008a93fe3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(image_path, model, processor):\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Process image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Model inference\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Extract predictions\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = model.post_process_object_detection(outputs, target_sizes=target_sizes)[0]\n",
    "    \n",
    "    # Filter results by confidence\n",
    "    threshold = 0.9\n",
    "    results = {key: value for key, value in results.items() if key == 'scores' and all(i >= threshold for i in value)}\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5c619ac-b50b-42c0-a8c3-743b7dc3c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(image_path, model, processor):\n",
    "    image = Image.open(image_path)\n",
    "    results = classify_image(image_path, model, processor)\n",
    "    \n",
    "    # Draw the image and the predicted boxes\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 10))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    scores = results.get('scores', [])\n",
    "    labels = results.get('labels', [])\n",
    "    boxes = results.get('boxes', [])\n",
    "\n",
    "    for score, label, box in zip(scores, labels, boxes):\n",
    "        x, y, width, height = box\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y, f'{model.config.id2label[label.item()]}: {score:.2f}', fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e74df4-619c-4000-8748-7943a00d0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"eccv_18_all_images_sm/58a1c8e3-23d2-11e8-a6a3-ec086b02610b.jpg\"\n",
    "show_predictions(image_path, model, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8b80e1d-e43c-40e7-86d6-d803008f8290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/detr.git\n",
      "  Cloning https://github.com/facebookresearch/detr.git to /tmp/pip-req-build-utfh5c5z\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detr.git /tmp/pip-req-build-utfh5c5z\n",
      "  Resolved https://github.com/facebookresearch/detr.git to commit 29901c51d7fe8712168b8d0d64351170bc0f83e0\n",
      "\u001b[31mERROR: git+https://github.com/facebookresearch/detr.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed63869-217c-48bb-b07b-39dcc6ad75b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
